# CLAUDE.md

This file provides guidance to Claude Code (claude.ai/code) when working with code in this repository.

## Project Overview

TachiBot MCP is a multi-model AI orchestration platform built as an MCP (Model Context Protocol) server. It provides 31+ tools for multi-model reasoning, YAML-based workflows, and intelligent tool routing across providers (Perplexity, Grok, OpenAI, Gemini, Qwen/Kimi via OpenRouter).

## Architecture Principles

### SOLID Principles

**Single Responsibility (SRP)**
- `src/utils/api-keys.ts` - Centralized API key resolution (single source of truth)
- Each tool file handles one provider: `grok-tools.ts`, `openai-tools.ts`, `gemini-tools.ts`
- `FocusModeRegistry` only manages mode registration
- `FocusToolService` only orchestrates mode execution

**Open/Closed Principle (OCP)**
- `FocusModeRegistry` - Add new focus modes without modifying the registry class
- `ModelProviderRegistry` - Register new models via `register()` without code changes
- Workflow YAML files - Add new workflows without touching TypeScript

**Liskov Substitution (LSP)**
- All `IFocusMode` implementations are interchangeable via the registry
- All `ITool` implementations follow the same contract

**Interface Segregation (ISP)**
- `IFocusMode` is minimal: `modeName`, `execute()`
- `ITool` is focused: `name`, `execute()`, optional `validate()`

**Dependency Inversion (DIP)**
- `FocusToolService` depends on abstract `FocusModeRegistry`, not concrete modes
- Services receive dependencies via constructor injection

### DRY (Don't Repeat Yourself)

**Apply DRY to:**
- API key resolution → use `src/utils/api-keys.ts` helpers
- Model-to-tool mappings → use `ModelProviderRegistry`
- Tool capability checks → use `ToolRouter`
- Shared helpers → `src/modes/shared/helpers/`

**Profiles are configuration, not logic:**
- Each profile in `src/profiles/*.ts` explicitly lists all tool booleans (true/false)
- This is intentional - profiles should be self-contained and easy to review at a glance
- The shared `ToolsConfig` interface in `types.ts` enforces the schema (that's the DRY part)
- Don't create "base profiles" with inheritance - keep each profile independent

## Build & Development Commands

```bash
# Build TypeScript and profiles
npm run build

# Run the MCP server
npm start                    # or: npm run dev

# Run tests
npm test                     # All tests
npm test -- --watch          # Watch mode
npm test -- challenger       # Single test file (e.g., challenger, verifier, scout)

# Inspect MCP tools
npm run inspect              # Uses fastmcp to inspect available tools

# Build MCP extension package
npm run package:extension    # Creates .mcpb file
```

## Architecture

### Core Entry Point
- `src/server.ts` - MCP server initialization, tool registration, FastMCP setup

### Key Subsystems

**Tool System** (`src/tools/`)
- Provider-specific tools: `perplexity-tools.ts`, `grok-tools.ts`, `openai-tools.ts`, `gemini-tools.ts`, `openrouter-tools.ts`
- `tool-router.ts` - Smart routing based on availability, cost, speed, and quality preferences
- `advanced-modes.ts` - Verifier, Challenger, Scout, Hunter tools

**Workflow Engine** (`src/workflows/`)
- `engine/WorkflowExecutionEngine.ts` - Core execution logic
- `engine/VariableInterpolator.ts` - `${variable}` and `${step.output}` interpolation
- `tool-mapper.ts` - Maps workflow step tool names to actual tool implementations
- Workflow files in `workflows/*.yaml` define multi-step AI processes

**Profile System** (`src/profiles/`, `profiles/`)
- TypeScript profiles in `src/profiles/` define tool configurations
- JSON profiles in `profiles/` are the built output (generated by `build:profiles`)
- Active profile set via `TACHIBOT_PROFILE` env var or `tools.config.json`

**Collaborative Orchestration** (`src/collaborative-orchestrator.ts`)
- Multi-model ping-pong reasoning sessions
- Model provider registry, tool execution service, visualization service

**Focus Modes** (`src/application/services/focus/`)
- Pluggable focus modes via `FocusModeRegistry`
- Modes: `tachibot-status`, `focus-deep`, plus legacy modes in server.ts switch statement

### Configuration
- `tools.config.json` - Active profile selection and custom tool toggles
- `.env` - API keys (PERPLEXITY_API_KEY, GROK_API_KEY, OPENAI_API_KEY, GEMINI_API_KEY, OPENROUTER_API_KEY)
- `src/config.ts` - Centralized configuration loading

### Model Configuration (DRY)
- `src/config/model-constants.ts` - **Single source of truth** for all model names
- `src/config/model-defaults.ts` - Tool-specific model selections (imports from constants)

**Strategy:** Always use latest models (quality over cost)

| Provider | Default Model | Notes |
|----------|---------------|-------|
| Gemini | `gemini-3-pro-preview` | Used everywhere (Nov 2025 latest) |
| OpenAI | `gpt-5.1-codex-mini` | Code tasks; `gpt-5.1` for deep reasoning |
| Grok | `grok-4-1-fast-reasoning` | Latest Grok 4.1 |

**To bump models:** Update `GEMINI_MODELS`, `GPT5_MODELS`, `GROK_MODELS` in `model-constants.ts`

### Validation System (`src/validators/`)
- Workflow validation: syntax, interpolation references, tool registry, dependency graphs
- Used by `validate_workflow` and `validate_workflow_file` tools

## Workflow YAML Structure

```yaml
name: workflow-name
description: What this workflow does
steps:
  - name: step-id
    tool: tool_name              # Must match registered MCP tool
    input:
      param: "${query}"          # Variable interpolation
    dependsOn: [other-step]      # Optional dependencies
    saveToFile: true             # Save output to workflow-output/
    maxTokens: 3500              # Token limit per step
    promptTechnique: analyze     # Optional: Apply prompt engineering technique
```

Workflow outputs go to `workflow-output/{workflow-name}/{timestamp}/`

## Prompt Engineering Techniques

Use `promptTechnique` in workflow steps to apply research-backed prompt patterns:

| Technique | Alias | Description |
|-----------|-------|-------------|
| `what_if_speculation` | `what_if` | Explore wild possibilities without limits |
| `alternative_perspectives` | `alt_view` | Analyze from 5 angles: child, scientist, artist, strategist, futurist |
| `creative_applications` | `creative_use` | Cross-domain creative applications |
| `innovative_solutions` | `innovate` | Generate unconventional solutions (3+ approaches) |
| `comprehensive_investigation` | `investigate` | 5W1H analysis + recent developments |
| `evidence_gathering` | `evidence` | Support, contradict, cases, stats, experts |
| `systematic_analysis` | `analyze` | Components → relationships → patterns → conclusions |
| `first_principles` | `first_prin` | Truths → assumptions → atomic units → rebuild |
| `feasibility_analysis` | `feasible` | Technical/economic/time/resources/risks/metrics |
| `quick_reflection` | `reflect` | Patterns, surprises, key insight, gaps, next steps |
| `pattern_recognition` | `patterns` | Themes, causality, cycles, anomalies |
| `problem_decomposition` | `decompose` | Core → sub-problems → dependencies → steps |
| `integration_reflection` | `integrate` | Synthesize convergent themes and meta-patterns |
| `council_of_experts` | `judge` | Multi-model council: perspectives → best elements → consensus → synthesis |

Implementation: `src/prompt-engineer-lite.ts`

## Claude Code Skills (Bundled)

TachiBot ships 7 skills in the `skills/` directory. These are deployed to `~/.claude/skills/` on install.

| Skill | Description | Key Tools Used |
|-------|------------|----------------|
| `/judge` | Multi-model council with fallback awareness | grok_search, perplexity_ask, grok_reason, kimi_thinking, openai_reason, gemini_analyze_text |
| `/think` | Sequential reasoning chains | nextThought |
| `/focus` | Mode-based multi-model reasoning | focus |
| `/breakdown` | Strategic decomposition pipeline (breadth-first) | execute_prompt_technique (first_principles, decompose, patterns, feasibility) |
| `/decompose` | Split into sub-problems, deep-dive each (depth-first) | kimi_decompose, nextThought chains |
| `/prompt` | Prompt engineering techniques | preview_prompt_technique, execute_prompt_technique |
| `/tachi` | Help & discovery | usage_stats |

All skills adapt to available API keys - no skill requires all providers.

## Adding New Tools

1. Create tool in appropriate `src/tools/*.ts` file
2. Add to `safeAddTool()` registration in `src/server.ts`
3. Add tool name to `src/profiles/types.ts` interface
4. Update all profile files in `src/profiles/`
5. Add to `tools.config.json` availableTools list
6. Run `npm run build` to regenerate profiles

## API Key Utilities

Use centralized helpers from `src/utils/api-keys.ts`:
```typescript
import { hasGrokApiKey, hasOpenAIApiKey, hasPerplexityApiKey, hasGeminiApiKey, hasOpenRouterApiKey } from "./utils/api-keys.js";
```

## Multi-Model Judgment Protocols

Use these patterns for complex reasoning with multiple AI models. **No new tools needed** - combine `nextThought`, `focus`, and Claude Code subagents.

### Tool Capabilities

| Tool | Purpose | Context |
|------|---------|---------|
| `nextThought` | Sequential thinking with model execution | `executeModel: true` calls models |
| `focus` | Mode-based reasoning (deep-reasoning, architecture-debate) | Orchestrates strategies |
| Task (subagents) | Isolated parallel execution | Context doesn't pollute main thread |

### nextThought Parameters

```typescript
nextThought({
  thought: "Analyze this problem",
  model: "gemini",           // grok, gemini, openai, perplexity, kimi, qwen
  executeModel: true,        // Actually call the model (not just log)
  contextWindow: 3,          // 3=last 3 thoughts, -1=ALL, 0=fresh
  nextThoughtNeeded: true    // Continue chain?
})
```

### Protocol 1: Parallel Council (Context-Free)

Spawn subagents for isolated parallel analysis, main thread stays clean:

```
Main Thread (orchestrator, <500 tokens)
    ├── Task: Subagent A (kimi - technical analysis)
    │   └── nextThought chain → returns JSON summary
    ├── Task: Subagent B (gemini - architecture review)
    │   └── nextThought chain → returns JSON summary
    └── Task: Subagent C (grok - edge case hunting)
        └── nextThought chain → returns JSON summary
            ↓
    Final Judge: gemini with contextWindow: -1 (sees ALL)
```

### Protocol 2: Sequential Pipeline

Each model builds on previous, progressive refinement:

```typescript
// Stage 1: Research (fresh context)
nextThought({ thought: "Research X", model: "perplexity", executeModel: true, contextWindow: 0 })
// Stage 2: Analyze (sees stage 1)
nextThought({ thought: "Analyze findings", model: "kimi", executeModel: true, contextWindow: 3 })
// Stage 3: Critique (sees stages 1-2)
nextThought({ thought: "Find flaws", model: "grok", executeModel: true, contextWindow: 3 })
// Stage 4: Judge (sees ALL)
nextThought({ thought: "Final verdict", model: "gemini", executeModel: true, contextWindow: -1 })
```

### Protocol 3: Adversarial Debate

Pro vs Con with isolated contexts:

```
Task: Subagent PRO (contextWindow: 0)
    └── "Argue FOR this approach" → summary

Task: Subagent CON (contextWindow: 0)
    └── "Argue AGAINST this approach" → summary

Main: nextThought({
  thought: "Judge debate",
  model: "gemini",
  contextWindow: -1,  // Sees both sides
  executeModel: true
})
```

### Protocol 4: Architecture Decision

Use focus tool for structured modes:

```typescript
focus({
  mode: "architecture-debate",
  query: "Microservices vs monolith for 10M users",
  models: ["grok", "gemini", "kimi"],
  rounds: 3
})
```

### Context Window Strategy

| Value | When to Use |
|-------|-------------|
| `0` (fresh) | Search, brainstorming, unbiased analysis |
| `3` (default) | Focused analysis, building on recent work |
| `-1` (ALL) | Final judge, synthesis, verdict |

### Model Aliases

User-friendly names that map to tools:
- `gemini judge` or `gemini-judge` → `gemini_analyze_text`
- `grok search` or `grok-search` → `grok_search`
- `grok reason` → `grok_reason`
- Normalization handles spaces/underscores/hyphens

### Best Practices

1. **Use subagents for heavy reasoning** - keeps main context clean
2. **Return structured JSON** from subagents, not raw text
3. **Use contextWindow: -1 only for final judge** - expensive but comprehensive
4. **Parallel > Sequential** when tasks are independent
5. **Fresh context (0) for search/brainstorm** - prevents bias
